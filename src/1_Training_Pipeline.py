# -*- coding: utf-8 -*-
"""stftrnn1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M1ety-OoROa6nzFs_tYlN87lZssTLEzx

DATASET EXTRACTION & PREPROCESSING
"""

import os
import zipfile
import shutil
import numpy as np
import librosa
from google.colab import drive
from tqdm import tqdm  # Progress bar

# =========================================================
# 1. MOUNT GOOGLE DRIVE
# =========================================================
drive.mount('/content/drive')

# =========================================================
# 2. KONFIGURASI PATH DAN DATASET
# =========================================================
# Lokasi folder dataset pada Google Drive
drive_folder = '/content/drive/MyDrive/datasetrnn1'

# Nama file ZIP dataset
zip_files = {
    'clean': 'clean_trainset_28spk_wav.zip',
    'noisy': 'noisy_trainset_28spk_wav.zip'
}

# Direktori kerja pada Google Colab
local_extract_path = '/content/data_raw'
processed_path = '/content/data_ready'

# =========================================================
# 3. PENYALINAN DAN EKSTRAK DATASET
# =========================================================
if not os.path.exists(local_extract_path):
    os.makedirs(local_extract_path)

for key, zip_name in zip_files.items():
    source = os.path.join(drive_folder, zip_name)
    dest = os.path.join('/content', zip_name)

    print(f"Menyalin file {zip_name} dari Google Drive.")

    if not os.path.exists(source):
        print(f"Kesalahan: File {zip_name} tidak ditemukan pada direktori {drive_folder}.")
        raise FileNotFoundError("File ZIP tidak ditemukan.")

    shutil.copy(source, dest)

    print(f"Mengekstrak file {zip_name}.")
    with zipfile.ZipFile(dest, 'r') as zip_ref:
        zip_ref.extractall(local_extract_path)

    # Menghapus file ZIP setelah proses ekstraksi
    os.remove(dest)

print("Proses penyalinan dan ekstraksi dataset telah selesai.")

# =========================================================
# 4. FUNGSI PREPROCESSING AUDIO
# =========================================================
def process_audio_folder(folder_path, n_fft=512, hop_length=256):
    # Mengumpulkan seluruh file WAV dalam folder dan subfolder
    wav_files = []
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.wav'):
                wav_files.append(os.path.join(root, file))

    # Pengurutan file untuk menjaga kesesuaian urutan clean dan noisy
    wav_files.sort()
    data_list = []

    print(f"Memproses {len(wav_files)} file audio dari direktori:")
    print(folder_path)

    for path in tqdm(wav_files):
        try:
            # Memuat audio dan melakukan resampling ke 16 kHz
            y, sr = librosa.load(path, sr=16000)

            # Transformasi Short-Time Fourier Transform (STFT)
            S = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)

            # Mengambil magnitude spektrum
            S_mag = np.abs(S)

            # Transpose agar berbentuk (time, frequency)
            data_list.append(S_mag.T)

        except Exception as e:
            print(f"Gagal memproses file {path}. Alasan: {e}")

    return data_list

# =========================================================
# 5. IDENTIFIKASI DIREKTORI DATA CLEAN DAN NOISY
# =========================================================
all_dirs = [x[0] for x in os.walk(local_extract_path)]
clean_folder = next((d for d in all_dirs if 'clean_trainset' in d), None)
noisy_folder = next((d for d in all_dirs if 'noisy_trainset' in d), None)

if not clean_folder or not noisy_folder:
    print("Kesalahan: Direktori dataset hasil ekstraksi tidak ditemukan.")
else:
    print(f"Direktori data clean: {clean_folder}")
    print(f"Direktori data noisy: {noisy_folder}")

    # =====================================================
    # 6. EKSEKUSI PREPROCESSING DAN PENYIMPANAN DATA
    # =====================================================
    clean_data = process_audio_folder(clean_folder)
    noisy_data = process_audio_folder(noisy_folder)

    os.makedirs(processed_path, exist_ok=True)

    print("Menyimpan data hasil preprocessing ke file .npy.")
    np.save(f'{processed_path}/clean_spectrograms.npy',
            np.array(clean_data, dtype=object))
    np.save(f'{processed_path}/noisy_spectrograms.npy',
            np.array(noisy_data, dtype=object))

    print("Proses preprocessing dan penyimpanan data telah selesai.")

"""MODEL TRAINING (LSTM)"""

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.model_selection import train_test_split
import numpy as np

# =========================================================
# 1. KONFIGURASI PELATIHAN
# =========================================================
BATCH_SIZE = 32
MAX_LEN = 128   # Panjang potongan spectrogram (sekitar 1 detik)
EPOCHS = 30     # Jumlah epoch pelatihan

# =========================================================
# 2. VERIFIKASI KETERSEDIAAN DATA DI MEMORI
# =========================================================
if 'clean_data' not in locals() or 'noisy_data' not in locals():
    print(
        "Kesalahan: Variabel data tidak ditemukan. "
        "Pastikan proses preprocessing telah dijalankan sebelumnya "
        "tanpa menyimpan ulang data ke berkas."
    )
else:
    print(f"Data tersedia. Jumlah data clean: {len(clean_data)}, noisy: {len(noisy_data)}")

# =========================================================
# 3. PEMBAGIAN DATA (TRAINING DAN VALIDASI)
# =========================================================
# Sebanyak 20% data digunakan sebagai data validasi
X_train, X_val, y_train, y_val = train_test_split(
    noisy_data,
    clean_data,
    test_size=0.2,
    random_state=42
)

print(f"Jumlah sampel data training: {len(X_train)}")
print(f"Jumlah sampel data validasi: {len(X_val)}")

# =========================================================
# 4. DATA GENERATOR UNTUK EFISIENSI MEMORI
# =========================================================
def data_generator(noisy_list, clean_list, batch_size=32):
    """
    Generator data untuk memberikan batch spectrogram
    secara bertahap guna menghemat penggunaan memori.
    """
    while True:
        X_batch, y_batch = [], []

        # Pemilihan indeks secara acak
        indices = np.random.randint(0, len(noisy_list), batch_size)

        for idx in indices:
            n_spec = noisy_list[idx]
            c_spec = clean_list[idx]

            # Pemotongan spectrogram secara acak agar panjang seragam
            if n_spec.shape[0] > MAX_LEN:
                start = np.random.randint(0, n_spec.shape[0] - MAX_LEN)
                n_cut = n_spec[start:start + MAX_LEN]
                c_cut = c_spec[start:start + MAX_LEN]

                X_batch.append(n_cut)
                y_batch.append(c_cut)

        if len(X_batch) > 0:
            yield np.array(X_batch), np.array(y_batch)

# =========================================================
# 5. PEMBANGUNAN ARSITEKTUR MODEL LSTM
# =========================================================
def build_model(input_shape):
    inputs = layers.Input(shape=input_shape)

    # LSTM layer pertama
    x = layers.LSTM(256, return_sequences=True)(inputs)
    x = layers.Dropout(0.2)(x)

    # LSTM layer kedua
    x = layers.LSTM(256, return_sequences=True)(x)
    x = layers.Dropout(0.2)(x)

    # Layer keluaran untuk merekonstruksi magnitude spectrogram
    outputs = layers.TimeDistributed(
        layers.Dense(257, activation='relu')
    )(x)

    model = models.Model(inputs=inputs, outputs=outputs)

    # Kompilasi model
    model.compile(
        optimizer='adam',
        loss='mae'
    )

    return model

# Bentuk input: (waktu, jumlah bin frekuensi)
model = build_model((MAX_LEN, 257))
model.summary()

# =========================================================
# 6. CALLBACKS PELATIHAN
# =========================================================
my_callbacks = [
    # Menghentikan pelatihan jika validation loss tidak membaik
    callbacks.EarlyStopping(
        patience=5,
        restore_best_weights=True,
        monitor='val_loss',
        verbose=1
    ),
    # Menyimpan model terbaik berdasarkan validation loss
    callbacks.ModelCheckpoint(
        'model_denoiser_best.h5',
        save_best_only=True,
        monitor='val_loss',
        verbose=1
    )
]

# =========================================================
# 7. PROSES PELATIHAN MODEL
# =========================================================
print("Proses pelatihan model dimulai.")

history = model.fit(
    data_generator(X_train, y_train, BATCH_SIZE),
    steps_per_epoch=len(X_train) // BATCH_SIZE,
    validation_data=data_generator(X_val, y_val, BATCH_SIZE),
    validation_steps=len(X_val) // BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=my_callbacks
)

print("Proses pelatihan telah selesai. Model terbaik disimpan sebagai 'model_denoiser_best.h5'.")

"""MODEL BACKUP SAVE TO GDRIVE"""

import shutil

shutil.copy('model_denoiser_best.h5', '/content/drive/MyDrive/datasetrnn1/MODEL_JADI_LSTM.h5')

print("Model berhasil disalin dan disimpan pada Google Drive.")